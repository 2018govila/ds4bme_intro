{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of notebook5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcaffo/ds4bme_intro/blob/master/notebooks/notebook5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIzXXVGuR9YM",
        "colab_type": "text"
      },
      "source": [
        "# Linear separable models\n",
        "\n",
        "We've now covered two ways to do prediction with a single variable, classification using logistic regression and prediction using a line and least squares. What if we have several predictiors? \n",
        "\n",
        "In both the logistic and linear regression models, we had a linear predictor, specifically, \n",
        "$$\n",
        "\\eta_i = \\beta_0 + \\beta_1 x_i.\n",
        "$$\n",
        "In the continuous case, we were modeling the expected value of the outcomes as linear. In the binary case, we were assuming that the naturual logarithm of the odds of a 1 outcome was linear. \n",
        "\n",
        "To estimate the unknown parameters, $\\beta_0$ and $\\beta_1$ we minimized\n",
        "$$\n",
        "\\sum_{i=1}^n || y_i - \\eta_i||^2 \n",
        "$$\n",
        "in the linear case and \n",
        "$$\n",
        "-\\sum_{i=1}^n \\left[\n",
        "  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n",
        "$$\n",
        "in the binary outcome case (where, recall, $\\eta_i$ depends on the parameters). \n",
        "\n",
        "We can easily extend these models to multiple predictors by assuming that the impact of the multiple predictors is linear and separable. That is,\n",
        "\n",
        "$$\n",
        "\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots \\beta_{p-1} x_{p-1,i}\n",
        "$$\n",
        "\n",
        "If we think about this as vectors and matrices, we obtain\n",
        "\n",
        "$$\n",
        "\\eta = X \\beta\n",
        "$$\n",
        "where $\\eta$ is an $n \\times 1$ vector, $X$ is an $n \\times p$ matrix with $i,j$ entry $x_{ij}$ and $\\beta$ is a $p\\times 1$ vector with entries $\\beta_j$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApSoCaMXb4x-",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the voxel-level data that we've been working with. First let's load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMMLqAkYRxb5",
        "colab_type": "code",
        "outputId": "b07bf0e0-ca56-47f4-fde6-abd7395fdc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.linear_model as lm\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels as sm\n",
        "\n",
        "## this sets some style parameters\n",
        "sns.set()\n",
        "\n",
        "## Download in the data if it's not already there\n",
        "! if [ ! -e oasis.csv ]; \\\n",
        "then wget https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv; \\\n",
        "fi;\n",
        "\n",
        "## Read in the data and display a few rows\n",
        "dat = pd.read_csv(\"oasis.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-23 18:19:04--  https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22274 (22K) [text/plain]\n",
            "Saving to: ‘oasis.csv’\n",
            "\n",
            "\roasis.csv             0%[                    ]       0  --.-KB/s               \roasis.csv           100%[===================>]  21.75K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-09-23 18:19:04 (1.77 MB/s) - ‘oasis.csv’ saved [22274/22274]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUih09BMdi9_",
        "colab_type": "text"
      },
      "source": [
        "Let's first try to fit the proton density data from the other imaging data. I'm going to use the `statsmodels` version of linear models since it has a nice format for dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onw6CyaCdrtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainFraction = .75\n",
        "\n",
        "sample = np.random.uniform(size = 100) < trainFraction\n",
        "trainingDat = dat[sample]\n",
        "testingDat = dat[~sample]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDJOnSsxe2N5",
        "colab_type": "code",
        "outputId": "a5c3bfcc-87a2-4081-92ed-3cdbec8651ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "results = smf.ols('PD ~ FLAIR + T1 + T2  + FLAIR_10 + T1_10 + T2_10 + FLAIR_20', data = trainingDat).fit()\n",
        "print(results.summary2())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Results: Ordinary least squares\n",
            "=================================================================\n",
            "Model:              OLS              Adj. R-squared:     0.732   \n",
            "Dependent Variable: PD               AIC:                71.0012 \n",
            "Date:               2019-09-23 18:19 BIC:                89.1026 \n",
            "No. Observations:   71               Log-Likelihood:     -27.501 \n",
            "Df Model:           7                F-statistic:        28.34   \n",
            "Df Residuals:       63               Prob (F-statistic): 3.38e-17\n",
            "R-squared:          0.759            Scale:              0.14318 \n",
            "------------------------------------------------------------------\n",
            "                Coef.   Std.Err.     t     P>|t|    [0.025  0.975]\n",
            "------------------------------------------------------------------\n",
            "Intercept       0.4725    0.1517   3.1153  0.0028   0.1694  0.7756\n",
            "FLAIR          -0.1419    0.0997  -1.4234  0.1596  -0.3412  0.0573\n",
            "T1             -0.1510    0.0961  -1.5710  0.1212  -0.3430  0.0411\n",
            "T2              0.7111    0.0948   7.5020  0.0000   0.5217  0.9005\n",
            "FLAIR_10       -0.0989    0.4045  -0.2444  0.8078  -0.9073  0.7096\n",
            "T1_10           0.1054    0.2007   0.5253  0.6012  -0.2957  0.5065\n",
            "T2_10          -0.0313    0.3338  -0.0938  0.9256  -0.6983  0.6357\n",
            "FLAIR_20        1.2095    0.8543   1.4158  0.1618  -0.4976  2.9167\n",
            "-----------------------------------------------------------------\n",
            "Omnibus:              0.653        Durbin-Watson:           1.959\n",
            "Prob(Omnibus):        0.722        Jarque-Bera (JB):        0.195\n",
            "Skew:                 0.038        Prob(JB):                0.907\n",
            "Kurtosis:             3.245        Condition No.:           45   \n",
            "=================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UrWqsZAfF1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f71dc845-1af2-426e-fb25-8aa74d972e6e"
      },
      "source": [
        "x = dat[['FLAIR','T1', 'T2', 'FLAIR_10', 'T1_10', 'T2_10', 'FLAIR_20']]\n",
        "y = dat[['GOLD_Lesions']]\n",
        "## Add the intercept column\n",
        "x = sm.tools.add_constant(x)\n",
        "\n",
        "xtraining = x[sample]\n",
        "xtesting = x[~sample]\n",
        "ytraining = y[sample]\n",
        "ytesting = y[~sample]\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
            "  return ptp(axis=axis, out=out, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaU1zUQluyFb",
        "colab_type": "code",
        "outputId": "1295842f-103f-4a93-8989-71062125a338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "fit = sm.discrete.discrete_model.Logit(ytraining, xtraining).fit()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.241546\n",
            "         Iterations 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clrxtIgQvDhJ",
        "colab_type": "code",
        "outputId": "2fc144fe-1bd0-450b-e180-5788b1ca6769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "fit.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>     <td>GOLD_Lesions</td>   <th>  No. Observations:  </th>  <td>    71</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    63</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     7</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Mon, 23 Sep 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.6515</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>18:22:05</td>     <th>  Log-Likelihood:    </th> <td> -17.150</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -49.206</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.266e-11</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th>    <td>   -4.7783</td> <td>    1.985</td> <td>   -2.407</td> <td> 0.016</td> <td>   -8.668</td> <td>   -0.888</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>FLAIR</th>    <td>    2.6983</td> <td>    1.353</td> <td>    1.994</td> <td> 0.046</td> <td>    0.046</td> <td>    5.351</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T1</th>       <td>    3.0484</td> <td>    1.306</td> <td>    2.334</td> <td> 0.020</td> <td>    0.489</td> <td>    5.608</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T2</th>       <td>    2.1443</td> <td>    1.196</td> <td>    1.793</td> <td> 0.073</td> <td>   -0.199</td> <td>    4.488</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>FLAIR_10</th> <td>    4.9421</td> <td>    4.256</td> <td>    1.161</td> <td> 0.246</td> <td>   -3.400</td> <td>   13.284</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T1_10</th>    <td>   -0.3192</td> <td>    2.331</td> <td>   -0.137</td> <td> 0.891</td> <td>   -4.888</td> <td>    4.250</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T2_10</th>    <td>   -4.9192</td> <td>    4.014</td> <td>   -1.226</td> <td> 0.220</td> <td>  -12.786</td> <td>    2.947</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>FLAIR_20</th> <td>  -16.6939</td> <td>    9.496</td> <td>   -1.758</td> <td> 0.079</td> <td>  -35.306</td> <td>    1.918</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:           GOLD_Lesions   No. Observations:                   71\n",
              "Model:                          Logit   Df Residuals:                       63\n",
              "Method:                           MLE   Df Model:                            7\n",
              "Date:                Mon, 23 Sep 2019   Pseudo R-squ.:                  0.6515\n",
              "Time:                        18:22:05   Log-Likelihood:                -17.150\n",
              "converged:                       True   LL-Null:                       -49.206\n",
              "Covariance Type:            nonrobust   LLR p-value:                 2.266e-11\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const         -4.7783      1.985     -2.407      0.016      -8.668      -0.888\n",
              "FLAIR          2.6983      1.353      1.994      0.046       0.046       5.351\n",
              "T1             3.0484      1.306      2.334      0.020       0.489       5.608\n",
              "T2             2.1443      1.196      1.793      0.073      -0.199       4.488\n",
              "FLAIR_10       4.9421      4.256      1.161      0.246      -3.400      13.284\n",
              "T1_10         -0.3192      2.331     -0.137      0.891      -4.888       4.250\n",
              "T2_10         -4.9192      4.014     -1.226      0.220     -12.786       2.947\n",
              "FLAIR_20     -16.6939      9.496     -1.758      0.079     -35.306       1.918\n",
              "==============================================================================\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkCEdWt0xGzn",
        "colab_type": "text"
      },
      "source": [
        "Now let's evaluate our prediction. Here, we're not going to classify as 0 or 1, but rather estimate the prediction. Note, we then would need to pick a threshold to have a classifier. We could use .5 as our threshold. However, it's often the case that we don't necessarily want to threshold at specifically that level. A solution for evalution is to plot how the sensitivity and specificity change by the threshold. \n",
        "\n",
        "In other words, consider the triplets\n",
        "$$\n",
        "(t, sens(t), spec(t))\n",
        "$$\n",
        "where $t$ is the threshold, `sens(t)` is the sensitivity at threshold $t$, `spec(t)` is the specificity at threshold `t`. \n",
        "\n",
        "Necessarily, the sensitivity and specificity \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxlcuTECxCyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phatTesting = fit.predict(xtesting)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaXw2nFg6UoO",
        "colab_type": "text"
      },
      "source": [
        "## Aside different python packages\n",
        "\n",
        "So far we've explored several plotting libraries including: default pandas methods, matplotlib, seaborn and plotly. We've also looked at several fitting libraries including to some extent numpy, but especially scikitlearn and statsmodels. What's the difference? Well, these packages are all mantained by different people and have different features and goals. For example, scikitlearn is more expansive than statsmodels, but statsmodels functions more like one is used to with statistical output. Matplotlib is very expansive, but seaborn has nicer default options and is a little easier. So, when doing data science with python, one has to get used to trying out a few packages, weighing the cost and benefits of each, and picking one. \n",
        "\n",
        "'statsmodels', what we're using above, has multiple methods for fitting binary models including: `sm.Logit`, `smf.logit`, `BinaryModel` and `glm`. Here I'm just going to use `Logit` which does not use the formula syntax of `logit`. Note, by default, this does not add an intercept this way. So, I'm adding a column of ones, which adds an intercept.\n",
        "\n",
        "Consider the following which uses the formula API\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj_2oMxvWC4S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "22d63911-ebca-429a-b14b-af07c39a59bb"
      },
      "source": [
        "results = smf.logit(formula = 'GOLD_Lesions ~ FLAIR + T1 + T2 + FLAIR_10 + T1_10 + T2_10 + FLAIR_20', data = trainingDat).fit()\n",
        "results.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.241546\n",
            "         Iterations 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>     <td>GOLD_Lesions</td>   <th>  No. Observations:  </th>  <td>    71</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    63</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     7</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Mon, 23 Sep 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.6515</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>18:23:47</td>     <th>  Log-Likelihood:    </th> <td> -17.150</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -49.206</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.266e-11</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>   -4.7783</td> <td>    1.985</td> <td>   -2.407</td> <td> 0.016</td> <td>   -8.668</td> <td>   -0.888</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>FLAIR</th>     <td>    2.6983</td> <td>    1.353</td> <td>    1.994</td> <td> 0.046</td> <td>    0.046</td> <td>    5.351</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T1</th>        <td>    3.0484</td> <td>    1.306</td> <td>    2.334</td> <td> 0.020</td> <td>    0.489</td> <td>    5.608</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T2</th>        <td>    2.1443</td> <td>    1.196</td> <td>    1.793</td> <td> 0.073</td> <td>   -0.199</td> <td>    4.488</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>FLAIR_10</th>  <td>    4.9421</td> <td>    4.256</td> <td>    1.161</td> <td> 0.246</td> <td>   -3.400</td> <td>   13.284</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T1_10</th>     <td>   -0.3192</td> <td>    2.331</td> <td>   -0.137</td> <td> 0.891</td> <td>   -4.888</td> <td>    4.250</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>T2_10</th>     <td>   -4.9192</td> <td>    4.014</td> <td>   -1.226</td> <td> 0.220</td> <td>  -12.786</td> <td>    2.947</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>FLAIR_20</th>  <td>  -16.6939</td> <td>    9.496</td> <td>   -1.758</td> <td> 0.079</td> <td>  -35.306</td> <td>    1.918</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:           GOLD_Lesions   No. Observations:                   71\n",
              "Model:                          Logit   Df Residuals:                       63\n",
              "Method:                           MLE   Df Model:                            7\n",
              "Date:                Mon, 23 Sep 2019   Pseudo R-squ.:                  0.6515\n",
              "Time:                        18:23:47   Log-Likelihood:                -17.150\n",
              "converged:                       True   LL-Null:                       -49.206\n",
              "Covariance Type:            nonrobust   LLR p-value:                 2.266e-11\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept     -4.7783      1.985     -2.407      0.016      -8.668      -0.888\n",
              "FLAIR          2.6983      1.353      1.994      0.046       0.046       5.351\n",
              "T1             3.0484      1.306      2.334      0.020       0.489       5.608\n",
              "T2             2.1443      1.196      1.793      0.073      -0.199       4.488\n",
              "FLAIR_10       4.9421      4.256      1.161      0.246      -3.400      13.284\n",
              "T1_10         -0.3192      2.331     -0.137      0.891      -4.888       4.250\n",
              "T2_10         -4.9192      4.014     -1.226      0.220     -12.786       2.947\n",
              "FLAIR_20     -16.6939      9.496     -1.758      0.079     -35.306       1.918\n",
              "==============================================================================\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}